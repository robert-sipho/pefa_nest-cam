
*File Structure*
```
├── Data
│     ├── 02_2015_prop.csv              <- data produced by 02 scripts
│     ├── 02_2016_prop.csv                 ...
│     ├── 02_2017_prop.csv                 ...
│     ├── 02_trt.csv                    <- list of supplemented sites
│     ├── 02_weather_1981-2019.csv      <- daily weather data from environment canada
│     ├──01_site_parquets               <- files produced by 01 script
│     │       ├──  1_2015.parquet 
│     │       └──  ...
│     └── 02_clean_parquets
│             ├── 02_alldata_2014.parquet <- cleaned data that shows a tally of all objects detected in every image
│             └── ...
├── Scripts 
│     ├──  01_convert2parquet.R          <- convert csv's to parquet. Csv's are in google drive, see script for link
│     ├──  02_data-clean                 <- cleaning script template
│     ├──  02a_data-clean_2017.R         <- cleaning script tailored to 2017
│     ├──  02b_data-clean_2016.R            ...
│     ├──  02c_data-clean_2015.R            ...
|     ├──  02b_data-clean_2014.R            ...
|     ├──  02b_data-clean_2013.R            ...
|     ├──  image_sampling_2016.R         <- sample images evenly across conditions and nests to build validation set
|     ├──  image_sampling_verification.R <- work with validation set to determine accuracy of manual/CNN predictions
│     └──  03_merge_and_model.R          <- merge all years generated by the above scripts (2a-2c), and model nest attendance
└── figures

```
<br />

<br />


## Process / scripts
### 01_convert2parquet

The combined size of all yolo outs is over 1 GB. To reduce size, I'm converting all csv's to Apache parquet format. Total file size is now ~165mb

### 02_data-clean

Converting raw yolo outputs into a useable format. I initially wanted to write one script that handled all irregularities in the data, but
given the nature of these irregularities, it's now my opinion that we need to be slow and methodical about how we clean data from each year. It will look ugly to have seperate scripts for each year, but I think it's the best move considering the unfortunate file naming conventions from some years, and odd directory structures in others.

I'm converting raw detections to daily proportions. We have multiple images for each day at a given nest, and each image either has an adult in it or not. 
I'm reducing this to a single value per day that indicates how many of the 1,440 minutes in each day contained an adult peregrine falcon in it. Excluding 2013 and 2014 to develop the model. Want to make sure the model is solid before I throw too much missing data into it

### Image sampling/model evaluation


#### Process
- [x] 1. Manually classify a validation set of images
- [x] 2. Use CNN to generate predictions on validation set
- [x] 3. compare CNN vs. manual classification
- [x] 4. Find images where classification differs, build new dataset with mistaken images
- [ ] 5. re-classify mistaken images to verify where the error originates (was yolo right, or was the human)
- [ ] 6. summarize results about yolo vs. human accuracy


### 03_merge_and_model
Merge model predictions into proportional nest attendance per day. Build model's to evaluate lag effects of inclement weather.
