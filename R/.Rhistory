nestling_info
nestling_info[,5] <- piv1[which(piv1$value == "nestling")+2,3]
nestling_info
nestling_info
nestling_info[,6] <- piv1[which(piv1$value == "nestling")+3,3]
nestling_info
nestling_info[,7] <- piv1[which(piv1$value == "nestling")+4,3]
nestling_info
nestling_info
piv1[which(piv1$value == "nestling"),]
piv1
nestling_info <- piv1[which(piv1$value == "nestling"),]
nestling_info[,4] <- piv1[which(piv1$value == "nestling")+1,3] # likelihood
nestling_info
nestling_info[,5] <- piv1[which(piv1$value == "nestling")+2,3] # leftx
nestling_info
nestling_info[,6] <- piv1[which(piv1$value == "nestling")+3,3] # topy
nestling_info
nestling_info[,7] <- piv1[which(piv1$value == "nestling")+4,3] # width
nestling_info
nestling_info[,8] <- piv1[which(piv1$value == "nestling")+5,3] # height
nestling_info
nestling_info <- nestling_info %>%
rename(lik = 4, leftx = 5, topy = 6, width = 7, height = 8) %>%
select(-value)
nestling_info
nestling_info$name <- str_replace(nestling_info$name, "Class", "nestling")
nestling_info
nestling_wide <- nestling_info %>%
pivot_wider(id_cols = Image_name, names_from = name, values_from = lik:height)
all_t
nestling_2016 <- all_t %>%
rename("Image_name" = "file") %>%
left_join(select(nestling_wide, Image_name, starts_with("lik")), by = "Image_name") %>%
mutate_at(vars(starts_with("lik")), ~replace_na(., 0))
all_t
nestling_2016 <- all_t %>%
left_join(select(nestling_wide, Image_name, starts_with("lik")), by = "Image_name") %>%
mutate_at(vars(starts_with("lik")), ~replace_na(., 0))
nestling_2016[,11:ncol(nestling_2016)] <- sapply(nestling_2016[,11:ncol(nestling_2016)],as.numeric)
nestling_2016
arrow::write_parquet(nestling_2016, "data/02_nestling_likelihoods_2016.parquet")
brood <- read_csv("data/03_daily_survival.csv") %>%
filter(year == 2016) %>%
filter(age == 1)
all_t %>% mutate(date = date(date)) %>% group_by(site, date) %>% summarize(sum = sum(adult), count = n()) %>% mutate(prop = sum/count) %>%
ggplot() +
geom_segment(aes(x = date, xend = date, y = 0, yend = count, group = site), size = 2) +
geom_point(data = brood, aes(x = date, y = 10),color = nuwcru::red2, size = 2) +
#scale_x_date(date_breaks = "2 weeks",
# date_labels = "%B %d") +
facet_grid(site~.) +
theme_nuwcru()
all_t %>% mutate(date = date(date)) %>% group_by(site, date) %>% summarize(sum = sum(adult), count = n()) %>% mutate(prop = sum/count) %>%
ggplot() +
geom_segment(aes(x = date, xend = date, y = 0, yend = count, group = site), size = 2) +
geom_point(data = brood, aes(x = date, y = 10),color = nuwcru::red2, size = 2) +
facet_grid(site~.) +
theme_nuwcru()
all_t %>% mutate(date = date(date)) %>% group_by(site, date) %>% summarize(sum = sum(adult), count = n()) %>% mutate(prop = sum/count) %>%
ggplot() +
geom_segment(aes(x = date, xend = date, y = 0, yend = count, group = site), size = 2) +
geom_point(data = brood, aes(x = date, y = 10),shape = 21, color = nuwcru::red2, size = 2) +
facet_grid(site~.) +
theme_nuwcru()
unique(all_t$site)
arrow::write_parquet(nestling_2016, "data/02_nestling_likelihoods_2016.parquet")
library(lubridate)
library(tidyverse)
library(readtext)
library(stringr)
library(nuwcru)
library(zoo)
library(nuwcru)
## Updated with meta data - December 30 2020 -
# Disclaimer ~~~~
# it would be really nice to have one script that can clean all years without edits
# Each year has unique ... eccentricities ... and it's my opinion that we should
# slowly clean each year to deal with the unique characteristics. If we do a better
# job of standardizing this pipeline (from reconyx in the field to computer), we can
# automate the data cleaning, but for now, we really need to be attentive and do this
# slowly/thoughtfully
# load data
clim <- read_csv("data/02_weather_1981-2019.csv") %>% select(-X1)
trt <- read_csv("data/00_trt.csv") %>% rename("site" = "NEST", "year" = "YEAR", "trt" = "TREATMENT")
trt$site <- as.character(trt$site)
# load meta
meta <- arrow::read_parquet("data/00_meta/clean_meta_2017.parquet")
# sergeant Drill should be an option to read parquets but I can't get it to connect.
# load files from specific year
f <- list.files("data/01_site_parquets/", full.names = TRUE, pattern = "*.parquet")
f <- f[str_detect(f, "2017")]
#f <- f[!str_detect(f, "19")] # remove sites 19 and 72, apparently some issues with labesl there.
#f <- f[!str_detect(f, "72")] # remove sites 19 and 72, apparently some issues with labesl there.
all_2017 <- do.call(rbind, lapply(f, arrow::read_parquet))
# *clean dates and site numbers  ---------------------------------------------
# dates - some missing in meta data, use filename
all_2017$Image_name <- str_replace(all_2017$Image_name, "/media/robert/", "")
meta$Image_name <- str_replace(meta$Image_name, "//", "/")
join <- all_2017 %>% left_join(meta, by = "Image_name")
join$date <- str_sub(join$Image_name,
str_locate(join$Image_name, "/201")[,1]+1,
str_locate(join$Image_name, ".JPG")[,1]-1)
join$date <- str_sub(join$date, 0, 19)
join$date <- parse_date_time(join$date, "y-m-d h-M-s")
all_2017 <- join
# some sites missing in meta_data, use filename
sites_missing <- all_2017 %>% filter(is.na(meta_site))
sites_not_missing <- all_2017 %>% filter(!is.na(meta_site))
sites_missing$meta_site <- str_sub(sites_missing$Image_name, str_locate(sites_missing$Image_name, "/Site")[,1]+1)
sites_missing$meta_site <- str_sub(sites_missing$meta_site, 0, str_locate(sites_missing$meta_site, "/")[,1]-1)
sites_missing$meta_site <- as.character(as.numeric(gsub(".*?([0-9]+).*", "\\1", sites_missing$meta_site)))
all_2017 <- rbind(sites_missing, sites_not_missing)
# *house keeping ----------------------------------------------------------
### all_2017$site <- str_sub(all_2017$Image_name, str_locate(tolower(all_2017$Image_name), "/site ")[,2]+1, str_locate(all_2017$Image_name, "/201")[,1]-1)
# an example of an odd characteristic... images have been Broken up into multiple directories
# per site
# remove post hatch, so resulting pictures are clumped purely by site
### all_2017$site <- str_replace(all_2017$site, "/Post-hatch", "")
### all_2017$date <- str_sub(all_2017$Image_name, str_locate(all_2017$Image_name, "/201")[,2]-3, str_locate(all_2017$Image_name, ".JPG")[,1]-1)
# remove motion trigger numbers
### all_2017$date <- str_sub(all_2017$date, 0, 19)
all_2017$yday  <- yday(all_2017$date)
all_2017$hour  <- hour(all_2017$date)
all_2017$month <- month(all_2017$date)
names(all_2017)
# reduce data frame to number of detections
all_t <- all_2017 %>% mutate(adult = rowSums(.[-38:-44] == "adult", na.rm = TRUE),
eggs  = rowSums(.[-38:-44] == "eggs", na.rm = TRUE),
nestling = rowSums(.[-38:-44] == "nestling", na.rm = TRUE),
sband  = rowSums(.[-38:-44] == "sband", na.rm = TRUE),
bband = rowSums(.[-38:-44] == "bband", na.rm = TRUE))  %>%
select(site = meta_site, date, month, yday, hour, adult, nestling, sband, bband)
x <- all_2017[which(all_t$nestling > 0),]
piv1 <- x %>%
select(Class1:height6, Image_name) %>%
mutate_at(vars(2:35), as.character) %>%
pivot_longer(Class1:height6,
values_transform = list(Percent1 = as.character))
nestling_info <- piv1[which(piv1$value == "nestling"),]
nestling_info[,4] <- piv1[which(piv1$value == "nestling")+1,3] # likelihood
nestling_info[,5] <- piv1[which(piv1$value == "nestling")+2,3] # leftx
nestling_info[,6] <- piv1[which(piv1$value == "nestling")+3,3] # topy
nestling_info[,7] <- piv1[which(piv1$value == "nestling")+4,3] # width
nestling_info[,8] <- piv1[which(piv1$value == "nestling")+5,3] # height
nestling_info <- nestling_info %>%
rename(lik = 4, leftx = 5, topy = 6, width = 7, height = 8) %>%
select(-value)
nestling_info$name <- str_replace(nestling_info$name, "Class", "nestling")
nestling_wide <- nestling_info %>%
pivot_wider(id_cols = Image_name, names_from = name, values_from = lik:height)
nestling_2017 <- all_t %>%
left_join(select(nestling_wide, Image_name, starts_with("lik")), by = "Image_name") %>%
mutate_at(vars(starts_with("lik")), ~replace_na(., 0))
all_t
all_2017 <- rbind(sites_missing, sites_not_missing)
all_2017
all_2017$yday  <- yday(all_2017$date)
all_2017$hour  <- hour(all_2017$date)
all_2017$month <- month(all_2017$date)
names(all_2017)
# reduce data frame to number of detections
all_t <- all_2017 %>% mutate(adult = rowSums(.[-38:-44] == "adult", na.rm = TRUE),
eggs  = rowSums(.[-38:-44] == "eggs", na.rm = TRUE),
nestling = rowSums(.[-38:-44] == "nestling", na.rm = TRUE),
sband  = rowSums(.[-38:-44] == "sband", na.rm = TRUE),
bband = rowSums(.[-38:-44] == "bband", na.rm = TRUE))  %>%
select(Image_name, site = meta_site, date, month, yday, hour, adult, nestling, sband, bband)
all_2017
x <- all_2017[which(all_t$nestling > 0),]
piv1 <- x %>%
select(Class1:height6, Image_name) %>%
mutate_at(vars(2:35), as.character) %>%
pivot_longer(Class1:height6,
values_transform = list(Percent1 = as.character))
nestling_info <- piv1[which(piv1$value == "nestling"),]
nestling_info[,4] <- piv1[which(piv1$value == "nestling")+1,3] # likelihood
nestling_info[,5] <- piv1[which(piv1$value == "nestling")+2,3] # leftx
nestling_info[,6] <- piv1[which(piv1$value == "nestling")+3,3] # topy
nestling_info[,7] <- piv1[which(piv1$value == "nestling")+4,3] # width
nestling_info[,8] <- piv1[which(piv1$value == "nestling")+5,3] # height
nestling_info <- nestling_info %>%
rename(lik = 4, leftx = 5, topy = 6, width = 7, height = 8) %>%
select(-value)
nestling_info$name <- str_replace(nestling_info$name, "Class", "nestling")
nestling_wide <- nestling_info %>%
pivot_wider(id_cols = Image_name, names_from = name, values_from = lik:height)
nestling_2017 <- all_t %>%
left_join(select(nestling_wide, Image_name, starts_with("lik")), by = "Image_name") %>%
mutate_at(vars(starts_with("lik")), ~replace_na(., 0))
nestling_2017[,11:ncol(nestling_2017)] <- sapply(nestling_2017[,11:ncol(nestling_2017)],as.numeric)
nestling_2017
arrow::write_parquet(nestling_2017, "data/02_nestling_likelihoods_2017.parquet")
brood <- read_csv("data/03_daily_survival.csv") %>%
filter(year == 2017) %>%
filter(age == 1)
all_t %>% mutate(date = date(date)) %>% group_by(site, date) %>% summarize(sum = sum(adult), count = n()) %>% mutate(prop = sum/count) %>%
ggplot() +
geom_segment(aes(x = date, xend = date, y = 0, yend = count, group = site), size = 2) +
geom_point(data = brood, aes(x = date, y = 10),shape = 21, color = nuwcru::red2, size = 2) +
facet_grid(site~.) +
theme_nuwcru()
arrow::write_parquet(nestling_2017, "data/02_nestling_likelihoods_2017.parquet")
names(nestling_2016)
nestling_2016
nestling_2016
y <- arrow::read_parquet(nestling_2016, "data/02_nestling_likelihoods_2016.parquet")
y <- arrow::read_parquet("data/02_nestling_likelihoods_2016.parquet")
head(y)
write.csv(nestling_2016, "/Volumes/GoogleDrive/My Drive/NuWCRU/Analysis/emhedlin/nuwcru/bnn/data/02_nestling_lik_2016.csv")
nestling_2016 <- all_t %>%
left_join(select(nestling_wide, Image_name, starts_with("lik")), by = "Image_name") %>%
mutate_at(vars(starts_with("lik")), ~replace_na(., 0))
nestling_2016[,11:ncol(nestling_2016)] <- sapply(nestling_2016[,11:ncol(nestling_2016)],as.numeric)
write.csv(nestling_2016, "/Volumes/GoogleDrive/My Drive/NuWCRU/Analysis/emhedlin/nuwcru/bnn/data/nestling_2016.csv")
write.csv(nestling_2015, "/Volumes/GoogleDrive/My Drive/NuWCRU/Analysis/emhedlin/nuwcru/bnn/data/nestling_2015.csv")
write.csv(nestling_2017, "/Volumes/GoogleDrive/My Drive/NuWCRU/Analysis/emhedlin/nuwcru/bnn/data/nestling_2017.csv")
brood <- read_csv("data/03_daily_survival.csv") %>%
filter(year == 2016)
brood
brood <- read_csv("data/03_daily_survival.csv") %>%
filter(year == 2016) %>%
group_by(date, site) %>%
summarize(bsize = sum(alive))
brood
brood <- read_csv("data/03_daily_survival.csv") %>%
filter(year == 2016) %>%
group_by(site, date) %>%
summarize(bsize = sum(alive))
brood
nestling_2017
read_csv("data/03_daily_survival.csv") %>%
filter(year == 2016) %>%
group_by(site, date) %>%
summarize(bsize = sum(alive)) %>%
right_join(nestling_2017, by = "date")
right_join(nestling_2017, by = c("site", "date")
read_csv("data/03_daily_survival.csv") %>%
filter(year == 2016) %>%
group_by(site, date) %>%
summarize(bsize = sum(alive)) %>%
right_join(nestling_2017, by = c("site", "date"))
nestling_2016
nestling_2016
nestling_2016 <- all_t %>%
left_join(select(nestling_wide, Image_name, starts_with("lik")), by = "Image_name") %>%
mutate_at(vars(starts_with("lik")), ~replace_na(., 0)) %>%
mutate(site = as.numeric(site))
read_csv("data/03_daily_survival.csv") %>%
filter(year == 2016) %>%
group_by(site, date) %>%
summarize(bsize = sum(alive)) %>%
right_join(nestling_2016, by = c("site", "date"))
d <- read_csv("/Volumes/GoogleDrive/My Drive/NuWCRU/Analysis/emhedlin/nuwcru/bnn/data/joined_data.csv")
write_parquet(d, "/Volumes/GoogleDrive/My Drive/NuWCRU/Analysis/emhedlin/nuwcru/bnn/data/joined_data.parquet")
arrow::write_parquet(d, "/Volumes/GoogleDrive/My Drive/NuWCRU/Analysis/emhedlin/nuwcru/bnn/data/joined_data.parquet")
# load data
clim <- read_csv("data/02_weather_1981-2019.csv") %>% select(-X1)
# trt <- read_csv("data/trt.csv") %>% rename("site" = "NEST", "year" = "YEAR", "trt" = "TREATMENT")
# trt$site <- as.character(trt$site)
# sergeant Drill should be an option to read parquets but I can't get it to connect.
# load files from specific year
f <- list.files("data/01_site_parquets/", full.names = TRUE, pattern = "*.parquet")
f <- f[str_detect(f, "2015")]
#f <- f[!str_detect(f, "19")] # remove sites 19 and 72, apparently some issues with labesl there.
#f <- f[!str_detect(f, "72")] # remove sites 19 and 72, apparently some issues with labesl there.
all_2015 <- do.call(rbind, lapply(f, arrow::read_parquet))
all_2015$Image_name <- str_replace(all_2015$Image_name, "/media/robert/", "")
all_2015
# *house keeping ----------------------------------------------------------
# load site names from meta data - use metadata_check.R
meta_2015 <- arrow::read_parquet("data/00_meta/clean_meta_2015.parquet")
all_2015 <- meta_2015 %>% right_join(all_2015, by = "Image_name")
all_2015$source_site <- as.factor(str_sub(all_2015$Image_name, str_locate(tolower(all_2015$Image_name), "/site ")[,2]+1, str_locate(all_2015$Image_name, "/201")[,1]-1))
all_2015$site <- ifelse(!is.na(all_2015$meta_site), all_2015$meta_site, all_2015$source_site)
all_2015 %>% filter(is.na(site)) %>% select(site, meta_site, source_site)
# Date --------------------------------------------------------------------
# there are NA's peppered throughout the data across sources, so this section is a bit involved
# the strategy will be to find NAs, and fill them with sources that actually have date info
# we
exif <- arrow::read_parquet("/Volumes/GoogleDrive/My Drive/NuWCRU/Analysis/NuWCRU/krmp_image-class/R/data/metadata/meta_2015.parquet")
exif <- exif %>% select(SourceFile, FileModifyDate)
exif$SourceFile <- str_replace(exif$SourceFile, "/Volumes/", "")
exif$SourceFile <- str_replace(exif$SourceFile, "//", "/")
# manipulate to make conversion to date easier
all_2015$meta_date <- str_replace_all(all_2015$meta_date, " AM", "")
all_2015$meta_date <- str_replace_all(all_2015$meta_date, " PM", "")
# all_2015$date <- lubridate::parse_date_time(all_2015$meta_date, "y-m-d h:M:s")
all_2015$date <- all_2015$meta_date
all_2015$date <- lubridate::parse_date_time(all_2015$date, "y-m-d h:M:s")
all_2015 %>% filter(is.na(date))
# Split all_2015 into 2, one df where dates are missing (a), and one where dates are present (b)
b <- all_2015 %>% filter(!is.na(date))
a <- all_2015 %>% filter(is.na(date))
# parse to date
a$date <- str_sub(a$Image_name,
str_locate(a$Image_name, "/2015")[,1]+1)
a$date <- str_replace_all(a$date, ".JPG", "")
a$date <- str_sub(a$date, 0, 19)
a$date <- lubridate::parse_date_time(a$date, "y-m-d h-M-s")
a$SourceFile <- a$Image_name
a$SourceFile <- str_replace(a$SourceFile, "//", "/")
join_exif_a <- a %>% inner_join(exif, by = "SourceFile")
join_exif_a$FileModifyDate <- str_sub(join_exif_a$FileModifyDate, 0, str_locate(join_exif_a$FileModifyDate, "-")[,1]-1)
join_exif_a$FileModifyDate <- lubridate::parse_date_time(join_exif_a$FileModifyDate, "y:m:d h:M:s")
# exif dates are all 1 hour behind of the original
join_exif_a$FileModifyDate  <- join_exif_a$FileModifyDate + hours(1)
join_exif_a$date <- join_exif_a$FileModifyDate
join_exif_a <- join_exif_a %>% select(-SourceFile, -FileModifyDate)
names(join_exif_a)
names(b)
test <- rbind(join_exif_a, b)
dim(test)
dim(all_2015)
all_2015 <- test
all_2015$yday <- yday(all_2015$date)
all_2015$hour <- hour(all_2015$date)
all_2015$month <- month(all_2015$date)
all_2015[,5:40]
# reduce data frame to number of detections
all_t <- all_2015 %>% mutate(adult    = rowSums(.[5:40] == "adult", na.rm = TRUE),
eggs     = rowSums(.[5:40] == "eggs", na.rm = TRUE),
nestling = rowSums(.[5:40] == "nestling", na.rm = TRUE),
sband    = rowSums(.[5:40] == "sband", na.rm = TRUE),
bband    = rowSums(.[5:40] == "bband", na.rm = TRUE))  %>%
select(site, date, month, yday, hour, adult, nestling, sband, bband, "file" = "Image_name") %>%
drop_na(site)
x <- all_2015[which(all_t$nestling > 0),]
piv1 <- x %>%
select(Class1:height6, Image_name) %>%
mutate_at(vars(2:35), as.character) %>%
pivot_longer(Class1:height6,
values_transform = list(Percent1 = as.character))
x <- all_2015[which(all_t$nestling > 0),]
piv1 <- x %>%
select(Class1:height6, Image_name) %>%
mutate_at(vars(2:35), as.character) %>%
pivot_longer(Class1:height6,
values_transform = list(Percent1 = as.character))
nestling_info <- piv1[which(piv1$value == "nestling"),]
nestling_info[,4] <- piv1[which(piv1$value == "nestling")+1,3]
nestling_info[,5] <- piv1[which(piv1$value == "nestling")+2,3]
nestling_info[,6] <- piv1[which(piv1$value == "nestling")+3,3]
nestling_info[,7] <- piv1[which(piv1$value == "nestling")+4,3]
nestling_info[,8] <- piv1[which(piv1$value == "nestling")+5,3]
nestling_info
nestling_info <- nestling_info %>%
rename(lik = 4, leftx = 5, topy = 6, width = 7, height = 8) %>%
select(-value)
nestling_wide <- nestling_info %>%
pivot_wider(id_cols = Image_name, names_from = name, values_from = lik:height)
nestling_wide
nestling_info
x <- all_2015[which(all_t$nestling > 0),]
piv1 <- x %>%
select(Class1:height6, Image_name) %>%
mutate_at(vars(2:35), as.character) %>%
pivot_longer(Class1:height6,
values_transform = list(Percent1 = as.character))
piv1
piv1[which(piv1$value == "nestling"),]
nestling_info <- piv1[which(piv1$value == "nestling"),]
piv1[which(piv1$value == "nestling")+1,3]
nestling_info[,4] <- piv1[which(piv1$value == "nestling")+1,3]
nestling_info[,5] <- piv1[which(piv1$value == "nestling")+2,3]
nestling_info[,6] <- piv1[which(piv1$value == "nestling")+3,3]
nestling_info[,7] <- piv1[which(piv1$value == "nestling")+4,3]
nestling_info[,8] <- piv1[which(piv1$value == "nestling")+5,3]
nestling_info
nestling_info <- nestling_info %>%
rename(lik = 4, leftx = 5, topy = 6, width = 7, height = 8) %>%
select(-value)
nestling_info
nestling_info$name <- str_replace(nestling_info$name, "Class", "nestling")
nestling_info
nestling_wide <- nestling_info %>%
pivot_wider(id_cols = Image_name, names_from = name, values_from = lik:height)
nestling_wide
nestling_wide <- nestling_info %>%
pivot_wider(id_cols = Image_name, names_from = name, values_from = lik:height) %>%
select(!starts_with("lik"))
nestling_wide
nestling_wide <- nestling_info %>%
pivot_wider(id_cols = Image_name, names_from = name, values_from = lik:height) %>%
select(!starts_with("lik")) %>%
get_x <- function(leftx, width){
return(leftx + (width/2))
}
get_x <- function(leftx, width){
return(leftx + (width/2))
}
get_x(10,20)
get_y <- function(topy, height){
return(topy - (height/2))
}
nestling_wide
nestling_wide$x1 <- ifelse(!is.na(nestling_wide$leftx_nestling1), get_x(nestling_wide$leftx_nestling1, nestling_wide$width_nestling1), "NA")
nestling_wide$x1 <- ifelse(!is.na(nestling_wide$leftx_nestling1), get_x(as.numeric(nestling_wide$leftx_nestling1), as.numeric(nestling_wide$width_nestling1)), "NA")
nestling_wide$x1
nestling_wide$x1 <- ifelse(!is.na(nestling_wide$leftx_nestling1), get_x(as.numeric(nestling_wide$leftx_nestling1), as.numeric(nestling_wide$width_nestling1)), NA)
nestling_wide$x1
names(nestling_wide)
nestling_wide$y1 <- ifelse(!is.na(nestling_wide$topy_nestling1), get_y(as.numeric(nestling_wide$topy_nestling1), as.numeric(nestling_wide$height_nestling1)), NA)
nestling_wide$y1
nestling_wide[929,]
View(nestling_wide[929,])
max(nestling_wide$topy_nestling1)
max(nestling_wide$topy_nestling1, na.rm = TRUE)
max(nestling_wide$topy_nestling2, na.rm = TRUE)
max(nestling_wide$topy_nestling3, na.rm = TRUE)
max(nestling_wide$topy_nestling4, na.rm = TRUE)
left <- nestling_wide$left_nestling4
nestling_wide$leftx_nestling1
left <- nestling_wide$topy_nestling1
left
max(left, na.rm = TRUE)
left <- nestling_wide$leftx_nestling1 + nestling_wide$width_nestling1
left <- as.numeric(nestling_wide$leftx_nestling1) + as.numeric(nestling_wide$width_nestling1)
max(left, na.rm = TRUE)
source <- "/Volumes/NUWCRU_DATA/agnico/"
dat <- read_exif(source, recursive= TRUE)
library(exifr)
dat <- read_exif(source, recursive= TRUE)
head(dat)
UserLabel <- dat %>% select("UserLabel")
library(tidyverse)
library(lubridate)
UserLabel <- dat %>% select("UserLabel")
UserLabel
unique(UserLabel)
dat
site <- dat %>%
select("SourceFile")
unique(site)
str_locate(dat$SourceFile, "Camera")[,2]
dat %>%
select("SourceFile") %>%
mutate(site = str_sub(., str_locate(dat$SourceFile, "Camera")[,2]+2))
library(tidyverse)
y13 <- read_csv("data/02_2013_prop.csv")
y13
unique(y13$site)
y16 <- read_csv("data/02_2016_prop.csv")
unique(y16$site)
read_csv("data/00_trt.csv")
trt <- read_csv("data/00_trt.csv")
trt16 <- trt %>%filter(year == 2016)
unique(trt16$nest)
y17 <- read_csv("data/02_2017_prop.csv")
trt17 <- trt %>%filter(year == 2017)
unique(trt16$nest)
trt17 %>% filter(nest == 145)
trt17
View(trt17)
library(lubridate)
library(tidyverse)
library(readtext)
library(stringr)
library(nuwcru)
library(zoo)
library(nuwcru)
# Disclaimer ~~~~
# it would be really nice to have one script that can clean all years without edits
# Each year has unique ... eccentricities ... and it's my opinion that we should
# slowly clean each year to deal with the unique characteristics. If we do a better
# job of standardizing this pipeline (from reconyx in the field to computer), we can
# automate the data cleaning, but for now, we really need to be attentive and do this
# slowly/thoughtfully
# load data
clim <- read_csv("data/02_weather_1981-2019.csv") %>% select(-X1)
trt <- read_csv("data/trt.csv") %>% rename("site" = "NEST", "year" = "YEAR", "trt" = "TREATMENT")
trt$site <- as.character(trt$site)
# sergeant Drill should be an option to read parquets but I can't get it to connect.
# load files from specific year
f <- list.files("data/01_site_parquets/", full.names = TRUE, pattern = "*.parquet")
f <- f[str_detect(f, "2013")]
#f <- f[!str_detect(f, "19")] # remove sites 19 and 72, apparently some issues with labesl there.
#f <- f[!str_detect(f, "72")] # remove sites 19 and 72, apparently some issues with labesl there.
all_2013 <- do.call(rbind, lapply(f, arrow::read_parquet))
head(all_2013)
# *house keeping ----------------------------------------------------------
all_2013$site <-str_sub(all_2013$Image_name, str_locate(tolower(all_2013$Image_name), "/site ")[,2]+1, str_locate(all_2013$Image_name, "/2013-")[,1]-1)
# an example of an odd characteristic... images have been Broken up into multiple directories
# per site
all_2013$date <- str_sub(all_2013$Image_name, str_locate(all_2013$Image_name, "/2013-")[,2]-4, str_locate(all_2013$Image_name, ".JPG")[,1]-1)
# remove motion trigger numbers
all_2013$date <- str_sub(all_2013$date, 0, 19)
all_2013$date <- parse_date_time(all_2013$date, "y-m-d h-M-s")
all_2013$yday <- yday(all_2013$date)
all_2013$hour <- hour(all_2013$date)
all_2013$month <- month(all_2013$date)
# reduce data frame to number of detections
all_t <- all_2013 %>% mutate(adult    = rowSums(.[-39] == "adult", na.rm = TRUE),
eggs     = rowSums(.[-39] == "eggs", na.rm = TRUE),
nestling = rowSums(.[-39] == "nestling", na.rm = TRUE),
sband    = rowSums(.[-39] == "sband", na.rm = TRUE),
bband    = rowSums(.[-39] == "bband", na.rm = TRUE))  %>%
select(site, date, month, yday, hour, adult, nestling, sband, bband) %>% drop_na(site) #%>%
all_2013
unique(all_2013$site)
unique(y16$site)
reticulate::repl_python()
reticulate::py_install("cv2")
reticulate::conda_list()
reticulate::py_install("cv2", pip = TRUE)
reticulate::py_install("opencv-python")
reticulate::py_install("opencv-python", pip = TRUE)
reticulate::repl_python()
reticulate::repl_python()
reticulate::repl_python()
reticulate::repl_python()
27 - 117
